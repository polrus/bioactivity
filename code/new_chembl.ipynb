{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, max, expr\n",
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/15 14:14:27 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/12/15 14:14:28 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/12/15 14:14:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/12/15 14:14:28 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dataframes(initial_df: DataFrame, \n",
    "                    second_df: DataFrame, \n",
    "                    initial_key_column: str, \n",
    "                    second_key_column: str,\n",
    "                    columns_to_join: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Joins two PySpark DataFrames on specified key columns.\n",
    "\n",
    "    Args:\n",
    "    initial_df (DataFrame): The initial PySpark DataFrame.\n",
    "    second_df (DataFrame): The second PySpark DataFrame to join with.\n",
    "    initial_key_column (str): The key column name in the initial DataFrame.\n",
    "    second_key_column (str): The key column name in the second DataFrame.\n",
    "    columns_to_join (list): List of column names from the second DataFrame to include in the join.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The resulting DataFrame after the join.\n",
    "    \"\"\"\n",
    "\n",
    "    # Selecting specified columns from the second DataFrame, including its key column\n",
    "    second_df_selected = second_df.select([second_key_column] + columns_to_join)\n",
    "\n",
    "    # Performing the left join\n",
    "    joined_df = initial_df.join(second_df_selected, \n",
    "                                initial_df[initial_key_column] == second_df_selected[second_key_column], \n",
    "                                how='left')\n",
    "\n",
    "    # Drop the second key column if not needed\n",
    "    joined_df = joined_df.drop(second_df_selected[second_key_column])\n",
    "\n",
    "    return joined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_values(df: DataFrame, column_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Count unique values in a specific column of a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The PySpark DataFrame.\n",
    "    column_name (str): The name of the column to analyze.\n",
    "\n",
    "    Returns:\n",
    "    int: The number of unique values in the column.\n",
    "    \"\"\"\n",
    "    # Get distinct values in the column and count them\n",
    "    unique_count = df.select(column_name).distinct().count()\n",
    "\n",
    "    return unique_count\n",
    "\n",
    "# Example usage\n",
    "# unique_count = count_unique_values(your_dataframe, 'your_column_name')\n",
    "# print(f\"Number of unique values: {unique_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_unique_values_and_counts(df: DataFrame, column_name: str):\n",
    "    \"\"\"\n",
    "    Shows unique values and their counts for a specified column in a Spark DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The Spark DataFrame to analyze.\n",
    "    column_name (str): The name of the column for which to count unique values.\n",
    "    \"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column {column_name} not found in DataFrame\")\n",
    "\n",
    "    unique_values_counts = df.groupBy(column_name).count()\n",
    "    unique_values_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|       drugId|\n",
      "+-------------+\n",
      "|CHEMBL1200632|\n",
      "|   CHEMBL1231|\n",
      "|CHEMBL1233511|\n",
      "|   CHEMBL1637|\n",
      "|CHEMBL1743017|\n",
      "| CHEMBL185885|\n",
      "|CHEMBL1949708|\n",
      "|CHEMBL2105675|\n",
      "|CHEMBL2107826|\n",
      "|CHEMBL2109673|\n",
      "|CHEMBL2346976|\n",
      "| CHEMBL279115|\n",
      "|CHEMBL3181832|\n",
      "|CHEMBL3545096|\n",
      "|CHEMBL3545103|\n",
      "|CHEMBL3545145|\n",
      "|CHEMBL3545312|\n",
      "| CHEMBL363648|\n",
      "|CHEMBL3707249|\n",
      "|CHEMBL3989766|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Take list of unique drugs (obtained from target with evidence in Platform or chemProbes)\n",
    "import pandas as pd\n",
    "\n",
    "unique_drugs_pd_df = pd.read_csv(\"../data/drug_to_target_unique_drugs.csv\")\n",
    "drug_list = spark.createDataFrame(unique_drugs_pd_df)\n",
    "# drug_list = spark.read.csv(drug_list_dir, header=True, inferSchema=True)\n",
    "drug_list.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=================================================>       (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of initial drugs:  12835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "in_drugs = count_unique_values(drug_list, \"drugId\")\n",
    "print(\"Number of initial drugs: \", in_drugs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each drug find a max phase of clinical trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of drugs by approval status: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:============================>                            (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|isApproved|count|\n",
      "+----------+-----+\n",
      "|      null| 4085|\n",
      "|      true| 3340|\n",
      "|     false| 5410|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "molecule_path = \"gs://open-targets-data-releases/23.12/output/etl/json/molecule\"\n",
    "molecule = spark.read.json(molecule_path)\n",
    "molecule.persist()\n",
    "\n",
    "# List of columns from molecule table\n",
    "list_molecule = [\"drugType\", \n",
    "                \"maximumClinicalTrialPhase\", \n",
    "                \"isApproved\", \n",
    "                \"linkedTargets\", \n",
    "                \"linkedDiseases\"]\n",
    "                 \n",
    "# Join list of drugs and max_phase from molecule table\n",
    "drug_list_phase = join_dataframes(drug_list, molecule, \"drugId\", \"id\", list_molecule).persist()\n",
    "\n",
    "# Show number of drugs by approval status\n",
    "print(\"Number of drugs by approval status: \")\n",
    "show_unique_values_and_counts(drug_list_phase, \"isApproved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each unique drug find bioactivity data from chembl_33_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique drugs from targets dataset:  12835\n",
      "Number of unique drugs with any bioactivities:  6215\n",
      "Number of unique targets with any drug bioactivities:  4848\n"
     ]
    }
   ],
   "source": [
    "activity_path = \"gs://open-targets-pre-data-releases/chembl-columns/chembl-inputs/chembl_33_activity.jsonl\"\n",
    "activity = spark.read.json(activity_path)\n",
    "activity.persist()\n",
    "\n",
    "# List of columns from activity table\n",
    "list_activity = [\"assay_chembl_id\",\n",
    "                \"assay_type\",\n",
    "                \"action_type\",\n",
    "                \"pchembl_value\",\n",
    "                \"standard_type\",\n",
    "                \"standard_units\",\n",
    "                \"standard_value\",\n",
    "                \"standard_relation\",\n",
    "                \"target_organism\",\n",
    "                \"target_pref_name\",\n",
    "                \"target_chembl_id\",\n",
    "                \"data_validity_comment\",\n",
    "                \"data_validity_description\"]\n",
    "                # \"standard_flag\",\n",
    "                # \"ligand_efficiency\",\n",
    "                # \"assay_variant_mutation\"\n",
    "                # \"assay_variant_accession\"\n",
    "\n",
    "# Join list of drugs and chembl_33_activity\n",
    "drug_to_activity = join_dataframes(drug_list_phase, activity, \"drugId\", \"molecule_chembl_id\", list_activity)\\\n",
    "                                    .filter(col(\"assay_chembl_id\").isNotNull()).persist()\n",
    "\n",
    "# Calculate for how many drugs and targets we have bioactivities\n",
    "drug_list_count = count_unique_values(drug_list, 'drugId')\n",
    "drug_to_activity_count = count_unique_values(drug_to_activity, 'drugId')\n",
    "drug_to_activity_count_targets = count_unique_values(drug_to_activity, 'target_chembl_id')\n",
    "\n",
    "print(\"Number of unique drugs from targets dataset: \", drug_list_count)\n",
    "print(\"Number of unique drugs with any bioactivities: \", drug_to_activity_count)\n",
    "print(\"Number of unique targets with any drug bioactivities: \", drug_to_activity_count_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each bioactivity assay find parameters from chembl_33_assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assay_path = \"gs://open-targets-pre-data-releases/chembl-columns/chembl-inputs/chembl_33_assay.jsonl\"\n",
    "assay = spark.read.json(assay_path)\n",
    "assay.persist()\n",
    "\n",
    "# List of columns from assay table\n",
    "list_assay = [\"confidence_score\",\n",
    "            \"confidence_description\",\n",
    "            \"assay_category\"]\n",
    "\n",
    "drug_to_assay = join_dataframes(drug_to_activity, assay, \"assay_chembl_id\", \"assay_chembl_id\", list_assay).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assay filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique drugs with bioactivities for non P and U assays: 6174\n",
      "Unique targets with bioactivities for non P and U assays: 4848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique drugs with bioactivities for single/homolog proteins: 3663\n",
      "Unique targets with bioactivities for single/homolog proteins: 2136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique drug-target pairs with bioactivities for human targets: 18860\n",
      "Unique drugs with bioactivities for human targets: 3373\n",
      "Unique targets with bioactivities for human targets: 1369\n"
     ]
    }
   ],
   "source": [
    "# assay_type\n",
    "assay_type_filter = drug_to_assay.filter(\n",
    "    (col(\"assay_type\") != \"P\") &\n",
    "    (col(\"assay_type\") != \"U\"))\n",
    "assay_type_d = count_unique_values(assay_type_filter, \"drugId\")\n",
    "assay_type_t = count_unique_values(assay_type_filter, \"target_chembl_id\")\n",
    "\n",
    "print(\"Unique drugs with bioactivities for non P and U assays:\", assay_type_d)\n",
    "print(\"Unique targets with bioactivities for non P and U assays:\", assay_type_t)\n",
    "\n",
    "# confidence_score\n",
    "confidence_score_filter = assay_type_filter.filter(col(\"confidence_score\").isin([9, 7]))\n",
    "confidence_score_d = count_unique_values(confidence_score_filter, \"drugId\")\n",
    "confidence_score_t = count_unique_values(confidence_score_filter, \"target_chembl_id\")\n",
    "\n",
    "print(\"Unique drugs with bioactivities for single/homolog proteins:\", confidence_score_d)\n",
    "print(\"Unique targets with bioactivities for single/homolog proteins:\", confidence_score_t)\n",
    "\n",
    "# target_organism\n",
    "target_organism_filter = confidence_score_filter.filter(col(\"target_organism\") == \"Homo sapiens\")\n",
    "target_organism_d = count_unique_values(target_organism_filter, \"drugId\")\n",
    "target_organism_t = count_unique_values(target_organism_filter, \"target_chembl_id\")\n",
    "drug_target_organism_t = target_organism_filter.count()\n",
    "\n",
    "print(\"Unique drug-target pairs with bioactivities for human targets:\", drug_target_organism_t)\n",
    "\n",
    "print(\"Unique drugs with bioactivities for human targets:\", target_organism_d)\n",
    "print(\"Unique targets with bioactivities for human targets:\", target_organism_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein classification by uniprot from SwissProt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Map target_chembl_id to uniprots via accession\n",
    "\n",
    "target_path = \"gs://open-targets-pre-data-releases/chembl-columns/chembl-inputs/chembl_33_target.jsonl\"\n",
    "target = spark.read.json(target_path)\n",
    "target.persist()\n",
    "\n",
    "list_target = [\"target_components\",\n",
    "            \"target_type\"]\n",
    "\n",
    "target_to_uniprot = join_dataframes(target_organism_filter, target, \"target_chembl_id\", \"target_chembl_id\", list_target).persist()\n",
    "\n",
    "# Explode target_components\n",
    "# Define a UDF to extract the 'accession' field\n",
    "def extract_accession(rows):\n",
    "    # Assuming you want to extract the 'accession' from the first Row object in the list\n",
    "    return rows[0].accession if rows else None\n",
    "\n",
    "# Register UDF\n",
    "extract_accession_udf = udf(extract_accession, StringType())\n",
    "\n",
    "# Apply UDF to create a new column with the 'accession' values\n",
    "target_to_uniprot_extr = target_to_uniprot.withColumn(\"accession\", extract_accession_udf(target_to_uniprot[\"target_components\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # What does protein_classifications mean?\n",
    "\n",
    "# target_component_path = \"gs://open-targets-pre-data-releases/chembl-columns/chembl-inputs/chembl_33_target_component.jsonl\"\n",
    "# target_component = spark.read.json(target_component_path)\n",
    "# target_component.persist()\n",
    "# target_component.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/15 14:15:47 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "proteinclass_path = pd.read_csv(\"../data/uniprot2family.csv\")\n",
    "proteinclass_str = proteinclass_path.astype(str).drop_duplicates()\n",
    "proteinclass = spark.createDataFrame(proteinclass_str)\n",
    "\n",
    "proteinclass_list = [\"proteinClass\"]\n",
    "\n",
    "uniprot_to_class = join_dataframes(target_to_uniprot_extr, proteinclass, \"accession\", \"accession\", proteinclass_list).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 119:===========================================>        (168 + 18) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|  proteinClass|count|\n",
      "+--------------+-----+\n",
      "|        Enzyme| 5476|\n",
      "|          GPCR| 1789|\n",
      "|        Kinase| 7317|\n",
      "|            TF|  209|\n",
      "|          None| 1664|\n",
      "|            IC|  359|\n",
      "|            NR|  660|\n",
      "|    Epigenetic|  769|\n",
      "|   Transporter|  583|\n",
      "|TF; Epigenetic|   29|\n",
      "|          null|    5|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "show_unique_values_and_counts(uniprot_to_class, 'proteinClass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18860"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniprot_to_class.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "uniprot_to_class.write.parquet(\"gs://ot-team/polina/uniprot_to_class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
